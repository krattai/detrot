http://www.linfo.org/create_shell_1.html

How to Create a First Shell Script

Shell scripts are short programs that are written in a shell programming language and interpreted by a shell process. They are extremely useful for automating tasks on Linux and other Unix-like operating systems.

A shell is a program that provides the traditional, text-only user interface for Unix-like operating systems. Its primary function is to read commands (i.e., instructions) that are typed into a console (i.e., an all-text display mode) or terminal window (i.e., all-text mode window) and then execute (i.e., run) them. The default shell on Linux is the very commonly used and highly versatile bash.

A programming language is a precise, artificial language that is used to write computer programs, which are sets of instructions that can be automatically translated (i.e., interpreted or compiled) into a form (i.e., machine language) that is directly understandable by a computer's central processing unit (CPU).

A feature of bash and other shells used on Unix-like operating systems is that each contains a built-in programming language, referred to as a shell programming language or shell scripting language, which is used to create shell scripts. Among the advantages of using shell scripts are that they can be very easy to create and that a large number are already available in books and on the Internet for use with or without modification for a wide variety of tasks. Shell scripts are also employed extensively in the default installations of Unix-like operating systems.

A First Script

The following example, although extremely simple, provides a useful introduction to creating and using shell scripts. The script clears the monitor screen of all previous lines and then writes the text Good morning, world. on it.

All that is necessary to create this script is to open a text editor (but not a word processor), such as gedit or vi, and type the following three lines exactly as shown on a new, blank page:

    #!/bin/bash
    clear
    echo "Good morning, world."

Alternatively, the above code could be copied from this page and pasted to a blank page opened by the text editor page using the standard keyboard or mouse copy and paste functions.

After saving this plain text file, with a file name such as morning (or anything else desired), the script is complete and almost ready to run. Scripts are typically run by typing a dot, a forward slash and the file name (with no spaces in between) and then pressing the ENTER key. Thus, for example, if the above script were saved with the name morning, an attempt could be made to execute it by issuing the following command:

    ./morning

However, the script probably will not run, in which case an error message will appear on the screen such as bash: ./morning: Permission denied. This is because the permissions for the file first have to be set to executable. (By default, the permissions for new files are set to read and write only.) The problem can easily be solved by using the chmod command with its 755 option (which will allow the file creator to read, write and execute the file) while in the same directory as that in which the file is located as follows:

    chmod 755 morning

Now the script is ready to run by typing the following, again while in the same directory, and then pressing the ENTER key:

    ./morning


How It Works

The first of the three lines tells the operating system what shell to use to interpret the script and the location (i.e., absolute pathname) of the shell. The shell is bash, which is located in the /bin directory (as are all shells); thus the line contains /bin/bash. This instruction is always preceded by a pound sign and an exclamation mark in order to inform the operating system that it is providing the name and location of the shell (or other scripting language).

The second line tells the shell to issue the clear command. This is a very simple command that removes all previous commands and output from the console or terminal window in which the command was issued.

The third line tells the shell to write the phrase Good morning, world. on the screen. It uses the echo command, which instructs the shell to repeat whatever follows it. (The quotation marks are not necessary in this case; however, it is good programming practice to use them, and they can make a big difference in more advanced scripts.) In slightly more technical terms, Good morning, world. is an argument (i.e., input data) that is passed to the echo command.

As is the case with other commands used in shell scripts, clear and echo can also be used independently of scripts. Thus, for example, typing clear on the screen and pressing the ENTER key would remove all previous commands and output and just leave a command prompt for entering the next command.

It Doesn't Work!

If the phrase Good morning, world. does not appear at the top of the screen, there are several possible reasons: (1) an error was made in copying the code (such as omitting the word echo), (2) the name used in the command was not exactly the same as that of the file (e.g., there is an extra space or a minor difference in spelling or capitalization), (3) the period and/or forward slash were omitted (or reversed) in the command, (4) a space was inserted after the period or slash, (5) the file is not a plain text file (typically because a word processor was used to create it instead of a text editor), (6) the command was not issued in the same directory as that in which the file is located and (7) the permissions were not changed to execute for the owner (i.e., creator) of the file.

It is important to avoid practicing writing and executing scripts as the root (i.e., administrative) user. An improperly written script could damage the operating system, and, in a worst case scenario, it could result in the loss of valuable data and make it necessary to reinstall the entire operating system. For this and other reasons, if an ordinary user account does not yet exist on the computer, one should immediately be created (which can be easily accomplished with a command such as adduser).

Experiments

There are a number of simple, and instructive, experiments that a curious user could do with the above example before moving on to more complex examples. They consist of revising the code as suggested below, saving the revisions (using either the same file name or a different file name), and then executing them as explained above.

(1) One is to try changing some of the wording (for example, changing the third line to echo "Good evening, folks.").

(2) Another is to add one or more additional lines to be written to the screen, each beginning with the word echo followed by at least one horizontal space.

(3) A third is to leave a blank line between two echo lines. (It will be seen that this will not affect the result; however, a blank line can be created by just typing echo on it and nothing else.)

(4) A fourth is to insert some blank horizontal spaces. (Notice that the result will be different depending on whether the blank spaces are inserted before or after the first quotation marks. This tells something about the role of quotation marks in shell scripts.)

(5) A fifth is to execute the file from a different directory from that in which it is located. This requires adding the path of the executable script to the beginning of the command name when it is issued (e.g., ./test/morning if the file has been moved to a subdirectory named test).

(6) Another experiment would be to add some other command to the script file, such as ps (which shows the processes currently on the system), pwd (which shows the current directory), uname (which provides basic information about a system's software and hardware) or df (which shows disk space usage). (Notice that these and other commands can be used in the script with any appropriate options and/or arguments.)






Created December 21, 2005.
Copyright Â© 2005 The Linux Information Project. All Rights Reserved.

~~~~~~~~~~~~~~~~~~

hi,

i have a simple script which runs wget to mirror a complete website

#!/bin/sh
wget -N options savepath url

this works fine.

i'd like to refine it to read a list of URLs from a file:

while read f
do
echo "url = $f"
`wget -N -P$SAVE_PATH $LOGIN $f`
done < $file

the script reads the file correctly and echoes out a list of URLs, but if i enclose wget in backquotes i get GNU: command not found. but if i leave the quotes off, then i get the standard blurb about wget echoed to my screen.

obv. no file is mirrored ;-)

how do i invoke wget in this instance?

many thanks!

<added> found it - it was all in the syntax. it needs backquotes and correct order of wget syntax:
`wget -N $LOGIN -P $SAVE_PATH $f`

thanks anyway :-)
tschild


#:913859 	 2:56 pm on Jan. 23, 2004 (utc 0)

I see you got a solution to the problem using your looping script.

Another solution would be to dispense with the loop and use the -i option of wget to specify a file listing the URLs to be retrieved.
jamie


#:913860 	 3:12 pm on Jan. 23, 2004 (utc 0)

hi tschild,

yeah i just found that out myself too ;-)

although the only way i could get the URLs from a file into the one var was:

while read f
do
var="$var $f"
done < $f

`wget -N $LOGIN -P $SAVE_PATH -I $var`

it works by concatting each line $f to the $var, separated by a space. i come from php so was searching in vain for $var .= $f but couldn't find it. is there such a thing in a shell script?

cheers 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Using wget, a shell script, and a dynamic filename created with the date command
By Alvin J. Alexander, devdaily.com

Here's a Unix/Linux shell script that I created to download a specific URL on the internet every day using the wget command. Note that I also use the date command to create a dynamic filename, which I'll describe shortly.

#!/bin/sh

# devdaily.com
# a shell script used to download a specific url.
# this is executed from a crontab entry every day.

DIR=/cygdrive/c/Al/Reports
FILE=dailyinfo.`date +"%Y%m%d"`
LOGFILE=wget.log
URL=http://foo.com/myurl.html

cd $DIR
wget $URL -O $FILE -o $LOGFILE

You don't need to write a script with all those variables, but in this case I did. (Also, that's not the real URL that I use.)

There are a couple of fun parts about the script. The first thing is that I create the output filename dynamically using the Unix/Linux date command. The portion of the code that looks like this:

FILE=dailyinfo.`date +"%Y%m%d"`

actually puts the string "dailyinfo.20070201" into the FILE variable. I like this because a) it gives me a unique filename every day, and b) it makes a large group of filenames easy to search and sort. The backticks surrounding the date command mean "run this command first, and put the output of the command right here". I use that syntax a lot in this situation, and also when I'm creating do/while loops in shell scripts.

Hopefully everything else is pretty straightforward. The weird thing about wget is that you have to specify the output file using an option, and can't just redirect the output. There may be an option that lets you write the output to STDOUT, but I didn't look for that, and just used the -O option when I found it.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Wonderful wget

Note: Please do not hit the server I mention in this article with wget unless you really want to download all those Jack Benny shows. I'm using the Jack Benny server as an example only. Please don't use it for practice.

The other night, I was presented with a challenge. I'd found a wonderful Web site called 'The Jack Benny Radio Archives'. The main draw of the Web site was the large number of shows available in mp3 format for downloading -- 362 mp3s, to be exact. I wanted those mp3s! However, the prospect of having to right-click on every mp3 hyperlink, choose 'Save Link Target As ...', and then click 'OK' to start the download did not appeal to me.

The mp3s were organized in a directory structure like this:

http://www.crispy.com/benny/mp3/
   season_10
   season_11
   season_12
   ...
   season_20
   season_21
   ...
   season_3
   season_4
   ...
   season_9

Note how the directories were not sorted in numerical order, as humans would do it, but in alphabetical order -- 1 does come before 3, after all.

Inside each directory were the mp3s. Some directories had just a few files in them, and some had close to 20. The listing of files in each directory looked like this:

  [BACK] Parent Directory        19-May-2002 01:03      - 
  [SND]  1944-12-24_532.mp3      06-Jul-2002 13:54   6.0M  
  [SND]  1944-12-31_533.mp3      06-Jul-2002 14:28   6.5M  
  [SND]  1945-01-07_534.mp3      06-Jul-2002 20:05   6.8M  
  [SND]  1945-01-14_535.mp3      06-Jul-2002 19:53   6.9M

The [SND] you see was actually an GIF image of musical notes that showed up in front of every file listing.

So the question was, how do I download all of these mp3s, mp3s which have different file names and which exist in different directories? Fortunately, wget was available.

wget is [free software/open source software] available from GNU. It runs on Linux, Unix, Mac OS X, and Windows. After downloading it, reading the manual, and installing it on my Windows 2000 machine, I opened the Command Prompt, as Windows 2000 calls it, and navigated to the directory into which I wanted my mp3s to reside.

d:
cd Music
mkdir Jack_Benny
cd Jack_Benny

In order, I went to my D drive, which is where I keep all my music, changed into the Music directory, made a new directory named 'Jack_Benny', and then changed into that new directory.

At this point, it was time to run wget. Here's the command I used to grab all those mp3s:

wget -r -l2 --no-parent -w -A.mp3 -R.html,.gif http://www.crispy.com/benny/mp3/

Let's decipher this command. [See footnote]

wget is the command I'm running, of course. At the far end is the URL that I want wget to use in its task, http://www.crispy.com/benny/mp3. The important stuff, though, lies in between the command and the URL.

-r stands for 'recursive'. A recursive command is one that follows links and goes down through directories in search of files. By telling wget that it is to act recursively, I can ensure that wget will go through every season's directory, grabbing all the mp3s it finds.

-l2 (that's the small letter L, by the way) tells wget how deep I want it to go in retriving files recursively. The 'l' stands for 'level' and the number is the depth. If I had specified '-l1', for 'level 1', then wget would look in the /mp3 directory only. That would result in no mp3s on my computer. Remember, the /mp3 directory contains other subdirectories: season_10, season_11, and so on, and those are directories that contain what I want. By specifying '-l2', I'm telling wget to grab all anything in /mp3 -- which will result in the all the season_# subdirectories -- and then go into each season_# directory in turn and grab anything in it. You need to be very careful with the level you specify. If you aren't careful, you can easily fill your hard drive in very little time!

--no-parent means just what it says: do not recurse into the parent directory. If you look back at the listing of files I demonstrated above, you'll note that the very first link is the Parent Directory. In other words, when in /season_10, the parent is /mp3. The same is true for /season_11, /season_12, and so on. I don't want wget to go *up*, I want it to go *down*. And I certainly don't need to waste time by going up into the same directory -- /mp3 -- every time I'm in a season's directory.

-w introduces a short wait between each file download. This helps prevent overloading the server as you hammer it continuously for files.

-A.mp3 tells wget that I wish it to only download mp3 files and nothing else. 'A' stands for 'accept', and it is followed by the file suffixes that I want, separated by commas. I only want one kind of file type -- mp3 -- so that is all I specify.

-R.html,.gif tells wget what I don't want: html and gifs. This way, I don't get those little musical notes represented by [SND] above. 'R' stands for 'reject'. Notice that I separated my list of suffixes with a comma.

After entering the command, I hit Enter and wget started its work. Results like the following flashed by in the command prompt window:

--05:33:32--  http://www.crispy.com/benny/mp3/season_8/1937-04-11_253.mp3
           => 'www.crispy.com/benny/mp3/season_8/1937-04-11_253.mp3'
Reusing connection to www.crispy.com:80.
HTTP request sent, awaiting response... 200 OK
Length: 7,154,990 [audio/mpeg]
  
  100%[====================================>] 7,154,990
  60.37K/s   ETA 00:00   05:35:28  (60.37 KB/s) -
  'www.crispy.com/benny/mp3/season_8/1937-04-11_253.mp3' saved
  [7154990/7154990]

About 5 1/2 hours later, wget was done. It had downloaded 22 folder containing 362 files, for a grand total of 2.19 gigabytes. Yes, gigabytes. You've got to admit, this is a lot easier that manually downloading all those mp3s!

And now, if you'll excuse me, I've got to burn some mp3s onto CDs, in anticipation of a very long, but now very entertaining, road trip I've got coming up. Enjoy!

Other wget resources

Smart (Script-Aided) Browsing ~ http://www.linuxjournal.com/article.php?sid=5905 ~ "Using the wget script and its mirroring option to download web pages."

Footnotes

By the way, you may be wondering why I didn't just use something like this:

wget -r -l2 http://www.crispy.com/benny/mp3/*.mp3

Unfortunately, that doesn't work, because attempting to retrieve files using HTTP, which is how these mp3s were made available, does not allow for the use of pattern matching. The only way is through the use of a command like the one I parsed above. 

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

help with wget script.
#!/bin/sh
# 'clear'

for i in $(seq -w 15 37)
do
echo $i
echo The content in Z
Z=`wget --dns-timeout=0.001 http://napdweb${i}.eao.abn-iad.ea.com:8000/webcore/test/test.jsp`
echo $Z
A="Connection timed out."
echo The content in A
echo $A
expr "$A" : '..\(...\)'
echo $A
done




Hi,Please see the above program.

The above program works for me but my problem is that i need to store a value in string once i get the output from "wget".

Moreover i need to find a sub-string for the obstained string.

Thanks in advance.
Reply With Quote
Remove advertisements
Sponsored Links
  #2 (permalink)  
Old 11-29-2008
KenJackson KenJackson is offline
Registered User
	  	

Join Date: May 2008
Location: Maryland, USA
Posts: 60
I couldn't access those servers but assuming you can, here is an example of how to use wget to read text from the web into an environment variable:
Code:

IP="$(wget -o/dev/null -O- http://jackson.io/ip/)"

The switch -o/dev/null throws away all the noisy information about the transfer. You could use -q instead.

The switch -O- (dash capital-O dash) pipes the output to stdout.

The syntax ="$( )" captures it. This is the same as using back-ticks, but I think this syntax is clearer.
Reply With Quote
  #3 (permalink)  
Old 11-29-2008
veerumahanthi41 veerumahanthi41 is offline
Registered User
	  	

Join Date: Nov 2008
Posts: 13
Hi , can anyone please help me. I am waiting for the answer.
HI can anyone help me, i am waiting for reply. since i have to complete my module it is urgent . can anyone help me please
Reply With Quote
  #4 (permalink)  
Old 11-30-2008
veerumahanthi41 veerumahanthi41 is offline
Registered User
	  	

Join Date: Nov 2008
Posts: 13
Help me
The content in Z before command
--19:17:12-- http://napdweb37.eao.abn-iad.ea.com:.../test/test.jsp
Resolving napdweb37.eao.abn-iad.ea.com... 10.120.244.76
Connecting to napdweb37.eao.abn-iad.ea.com|10.120.244.76|:8000... connected.
HTTP request sent, awaiting response... 200 OK
Length: 67 [text/html]
Saving to: `STDOUT'
100%[==================================================================================================== ================>] 67 --.-K/s in 0s
19:17:13 (3.36 MB/s) - `-' saved [67/67]
<html> <head> </head> <body> Test JSP page. </body> </html>
The content in Z after command
<ht


THis is the output i am getting, but i want to do substring operation on full ouput. I am able to acess only that html output . Can anyone help me how can i acess full output.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

How to script wget in bash?
The script below is giving me grief! The error message says

Code:

/download.bash: line 16: syntax error near unexpected token `else'
./download.bash: line 16: `else wget "http://downloads.sourceforge.net/hibernate/hibernate-3.2.5.ga.zip?modtime=1185893922&big_mirror=1"
'

I think it must be a problem with the special characters in the URL. Can some make this script work?
Thanks,
Siegfried

Code:

#!/usr/bin/bash
#
# Begin commands to execute this file using bash with bash
# chmod +x download.bash
# ./download.bash
# End commands to execute this file using bash with bash
#
# $Log: download.bash,v $
# Revision 1.1  2007/12/25 05:48:31  Administrator
# Initial revision
#
#
if [ -e hibernate-3.2.5.ga.zip ]
then
echo "already got hibernate-3.2.5.ga.zip"
else wget "http://downloads.sourceforge.net/hibernate/hibernate-3.2.5.ga.zip?modtime=1185893922&big_mirror=1"
fi

Reply With Quote
Remove advertisements
Sponsored Links
  #2 (permalink)  
Old 12-25-2007
DeepakS's Avatar 	
DeepakS DeepakS is offline
Registered User
	  	

Join Date: Aug 2006
Posts: 62
Works for me. Possibly a control character showed up when you copied the URL. Copy your forum text into a new script to see.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Make Wget cater to your needs
By Aleksey 'LXj' Alekseyev on January 12, 2007 (8:00:00 AM)

Share    Print    Comments   
Most Linux users are familiar with using GNU Wget to download single files by passing the URL as an argument to the wget command, but you can also use Wget with desktop applications. It requires a little preparation, but it's easy to integrate Wget with your favorite browser and other desktop applications. You can also use Wget in scripts to categorize batch downloads and make them fault-tolerant. Here's how to get Wget to sit up and beg for you.

If you have a list of files you want to download, you can use Wget's -i option, which tells Wget to read a list of URLs from a file. Invoke wget -i filelist and wait until it finishes the job, and your files are downloaded!

Most download managers, when you pause downloading, you close the connection to the server and open it again when you choose to resume. When you download a file using Wget, you can pause by pressing Ctrl-Z, and the connection will not be lost if you resume quickly enough (the connection usually times out after 60 seconds). That means you don't lose time when reconnecting.

If you stop Wget before it has finished downloading the list of files, you may want to continue from the last file it was downloading. In that case, using wget -i filelist won't do the job anymore. What you need is a script that will delete a URL from the list after Wget finishes downloading the appropriate file. This short script will do the job:

#!/bin/sh
# wget-list: manage the list of downloaded files

# invoke wget-list without arguments

while [ `find .wget-list -size +0` ]
 do
  url=`head -n1 .wget-list`
   wget -c $url
   sed -si 1d .wget-list
 done

Segmented downloading

Some download managers support segmented downloading, which means downloading several pieces of file simultaneously. Segmented downloading is supposed to help utilize bandwidth more efficiently, but this is not always true: if your connection speed is not high, you will create more traffic, but downloading files will not be faster. For that reason, some webmasters ban the use of segmented downloading (though this is rare).

Single-threaded downloading has its benefits, especially when Wget is concerned. Other download managers have internal databases to help them keep track of which parts of files are already downloaded. Wget gets this information simply by scanning a file's size. This means that Wget is able to continue downloading a file which another application started to download; most other download managers lack this feature. Usually I start by downloading a file with my browser, and if it is too large, I stop downloading and finish it later with Wget.

Still want to try the segmented downloading? The Aria2 console download utility supports it.

With this technique, you store the list of URLs in a file called .wget-list, one URL per line. On each line you can not only write URLs but also additional options for Wget. For example, if you want to set the name of the output file, you can add a line like <URL> -O <filename> to .wget-list, where -O is a Wget command-line option and <filename> is the the name you want it to use. You can add the -c option to be sure that the download will be continued from the same place Wget (or another application) stopped at. Consult the wget manpage for other options.

When Wget is finished downloading the first file in the list, the first line of .wget-list is deleted, so on the next loop Wget starts downloading the next file in list. If you press Ctrl-C, the next time you run wget-list it will continue downloading the same file.

If you want to categorize the files you download, you could create several directories to place files in, such as src, movie-trailers, and docs. Create a file .wget-list in each directory, and use a master script like wget-all below to process the .wget-list files in each subdirectory:

#/bin/sh
# wget-all: process .wget-list in every subdirectory
# invoke wget-all without arguments

find -name .wget-list -execdir wget-list ';'

This script looks for files named .wget-list and executes the command wget-list in every directory where it found the file.

If you want to set priorities between the categories, to specify which will be processed first, you need to be able to specify the order to work on the directories, as in wget-dirs:

#!/bin/sh
# wget-dirs: run wget-all in specified directories
# invoking: wget-dirs <path-to-directory> ...

for dir in $*
  do
      pushd $dir
      wget-all
      popd
  done
wget-all

This script should be executed with parameters: if you want to download files in the src directory, and then files in the docs directory, you should invoke wget-dirs src docs (don't forget to change the current directory to the one containing those directories, or else specify the full paths). In this script pushd changes the current directory and remembers the previous one in its stack, and popd changes the current directory to the last remembered one.

Desktop integration

Now you need an easy way of adding URLs to list. You can use this add-url script to add a URL to the .wget-list category:

#!/bin/sh
# add-url: add URL to list

# invoking: add-url URL

echo $* >>~/download/.wget-list
#  assuming that ~/download is the directory for downloaded files

Add-url is a handy script if you're at the command line, but KDE users can take more advantage of it by using Klipper's ability to run commands on any string copied to the clipboard. Open the configuration dialog by right-clicking on the Klipper icon in the system tray or the Klipper applet, and choose Configure Klipper, and go to the Actions tab. You will notice that you can set different groups of actions for strings matching different regular expressions.

There should already be a group for HTTP links ("^https?://."). Right-click on this group and choose Add Command, then type "add-url %s" for the command and "Add URL to download queue" for the description. Then go to Global Shortcuts tab and select a shortcut to invoke the action. From then on, every time you use this shortcut, you will see a menu of actions available for the string currently in clipboard, which will now include the item for running the script you prepared to add URLs to the Wget queue.

Klipper helps you to automate adding URLs from any application, but most of the time you will grab URLs from the browser, so why not add an item to its context menu?

The FlashGot for Firefox extension helps you to integrate any download manager into Firefox. After downloading and installing FlashGot, select FlashGot -> Settings from Firefox's Tools menu. Enter the path of the add-url script, and leave the URL template as "[URL]". Now you can use FlashGot's context menu items, including "Download the link via FlashGot" and "Download everything via FlashGot," to download files with Wget.

Opera users can also use Wget as a download manager. In the main Opera menu select Tools -> Preferences. Go to the Advanced tab, select Toolbars in the list at the left side. Click on Opera Standard in Menu Setup and click on Duplicate. Don't close the dialog, just minimize the Opera main window. Now open the file ~/.opera/menu/standard_menu (1).ini and add this line to the Link Popup Menu and Image Link Popup Menu sections:

Item, "Add to download queue"="Execute program, "/home/user/bin/add-url","%l""

This assumes that /home/user/bin/add-url is the full path to add-url -- don't use ~ there.

Now restore the Opera window, select the Copy of Opera Standard menu setup, and click OK. You should notice the new items in the context menu when you right-click.

Those are several ways that an "old-style" command-line tool like Wget can be easily integrated into a GUI environment. If you are a fan of GUI tools, you can also use Wget front ends such as Gwget for GNOME and KGet for KDE.

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


	09/30/04 04:05 PM, ID: 12195670
	
khkremer:
Try this:

#!/bin/sh

url_list="http://url1 http://url2"

failed=0
failed_urls=""
success=0
success_urls=""

for i in $url_list; do
        wget -q $i
        res=$?
        if [ $res -eq 0 ] ; then
                (( success++ ))
                success_urls="$success_urls $i"
        fi
        if [ $res -ne 0 ] ; then
                (( failed++ ))
                failed_urls="$failed_urls $i"
        fi
done


echo "$success successful downloads:"
for i in $success_urls ; do
        echo $i
done
echo "$failed failed downloads"
for i in $failed_urls ; do
        echo $i
done
# end of script

Set the url_list variable to the URLs you want to download. You can of course also change the wget parameters.
Accepted Solution
	 	
	10/01/04 11:21 AM, ID: 12202656
	
GR999:
Wow nice job! Works great just modified the url list stuff. Much better than what i had originally as I am fairly new to scripting. This is what I had started with...bear with me I only have a very small grasp of programming:

#!/bin/bash

SAVED=0
PKGS=0


echo "What file should I use?"
read urllist

for i in $(cat $urllist); do
wget $i
 if $? > 0
 then PKGS=$[$PKGS + 1]
 elif $? = 0
 then
        SAVED=$[$SAVED + 1]
        PKGS=$[$PKGS + 1]
 fi

done

echo "The files successfully downloaded is $SAVED/$PKGS "

##End Script

This would do its job, but the last echo would not be accurate. Anyway I appreciate your help, thnx alot!!

GR
